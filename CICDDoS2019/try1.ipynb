{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eac2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8322ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_a_s=list(range(20000))\n",
    "random.shuffle(global_a_s)\n",
    "global_a_count={i:0 for i in ['DNS', 'LDAP', 'MSSQL', 'NetBIOS', 'NTP', 'Portmap', 'SNMP', 'SSDP', 'Syn', 'TFTP', 'UDPLag', 'UDP']}\n",
    "global_n_s=torch.randperm(73500)\n",
    "global_n_count=0\n",
    "\n",
    "global_a_oe_count={i:0 for i in ['DNS', 'LDAP', 'MSSQL', 'NetBIOS', 'NTP', 'Portmap', 'SNMP', 'SSDP', 'Syn', 'TFTP', 'UDPLag', 'UDP']}\n",
    "\n",
    "def sample_anomaly_from_mix(type_a,num_a):\n",
    "    global global_a_s\n",
    "    global global_a_count\n",
    "    o_data_x=torch.load(\"./../../dataset/processed_CICDDoS2019/result5/attack_mix/\"+type_a+\".pt\")\n",
    "    o_index=global_a_s[global_a_count[type_a]:global_a_count[type_a]+num_a]\n",
    "    global_a_count[type_a]+=num_a\n",
    "    return o_data_x[o_index]\n",
    "\n",
    "def sample_anomaly_from_oe(type_a,num_a):\n",
    "    global global_a_oe_count\n",
    "    assert num_a<=30\n",
    "    o_data_x=torch.load(\"./../../dataset/processed_CICDDoS2019/result5/oe/\"+type_a+\"_oe.pt\")\n",
    "    t=o_data_x[global_a_oe_count[type_a]:global_a_oe_count[type_a]+num_a]\n",
    "    global_a_oe_count[type_a]+=num_a\n",
    "    return t\n",
    "\n",
    "def load_normal(num_n):\n",
    "    # 56651+25707+13273\n",
    "    global global_n_s\n",
    "    global global_n_count\n",
    "    n_data_x=torch.load(\"./../../dataset/processed_CICDDoS2019/result5/BENIGN/BENIGN_train.pt\")\n",
    "    \n",
    "    global_n_count+=num_n\n",
    "    return n_data_x[global_n_s[global_n_count-num_n:global_n_count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_client_data(dirty_d,clean_d=(0,{})):\n",
    "    # 返回的result包含以下内容\n",
    "    # result[0][0]是污染数据集的输入\n",
    "    # result[0][1]是污染数据集的实际标签(仅作为辅助信息 训练时不应使用)\n",
    "    # result[1][0]是小型干净数据集的异常输入部分\n",
    "    # result[1][1]是小型干净数据集的正常输入部分\n",
    "    # result[2]是数据总数\n",
    "    dirty_normal_num=dirty_d[0]\n",
    "    clean_normal_num=clean_d[0]\n",
    "    normal_d=load_normal(dirty_normal_num+clean_normal_num)\n",
    "    dirty_data_x=normal_d[:dirty_normal_num]\n",
    "    clean_data_x=normal_d[dirty_normal_num:]\n",
    "    \n",
    "    total_dirty_anomaly_num=0\n",
    "    for k in dirty_d[1].keys():\n",
    "        dirty_data_x=torch.cat((dirty_data_x,sample_anomaly_from_mix(k,dirty_d[1][k])),dim=0)\n",
    "        total_dirty_anomaly_num+=dirty_d[1][k]\n",
    "    dirty_data_y=torch.cat((torch.zeros(dirty_normal_num),torch.ones(total_dirty_anomaly_num)),dim=0).float()\n",
    "    \n",
    "    tc=torch.zeros(0,66).float()\n",
    "    total_clean_anomaly_num=0\n",
    "    for k in clean_d[1].keys():\n",
    "        tc=torch.cat((tc,sample_anomaly_from_oe(k,clean_d[1][k])),dim=0)\n",
    "        total_clean_anomaly_num+=clean_d[1][k]\n",
    "\n",
    "    return [[dirty_data_x,dirty_data_y],[tc,clean_data_x],\n",
    "             dirty_normal_num+clean_normal_num+total_dirty_anomaly_num+total_clean_anomaly_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabTransformNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TabTransformNet, self).__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            # nn.Linear(79, 64,bias=False),\n",
    "            nn.Linear(66, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 66),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "\n",
    "class NeuTraL(nn.Module):\n",
    "    def __init__(self, num_trans):\n",
    "        super(NeuTraL, self).__init__()\n",
    "        self.enc=nn.Sequential(\n",
    "            nn.Linear(66, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "        )\n",
    "        self.num_trans=num_trans\n",
    "        self.trans = nn.ModuleList([TabTransformNet() for _ in range(self.num_trans)])\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_T = torch.zeros(x.shape[0],self.num_trans,x.shape[-1]).to(x)\n",
    "        for i in range(self.num_trans):\n",
    "            mask = self.trans[i](x)\n",
    "            mask = torch.sigmoid(mask)\n",
    "            x_T[:, i] = mask * x\n",
    "        x_cat = torch.cat([x.unsqueeze(1),x_T],1)  # x_cat形状为[batch_size,self.num_trans+1,79]\n",
    "        zs = self.enc(x_cat.reshape(-1,x.shape[-1]))\n",
    "        z_dim=zs.shape[-1]\n",
    "        return zs.reshape(x.shape[0],self.num_trans+1,z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuTraL_loss(z,device,y,temp=0.1):\n",
    "    z=z.to(device)\n",
    "    z = nn.functional.normalize(z, p=2, dim=-1)\n",
    "    # 上面这一行 z本来形状为[batch_size,num_trans+1,z_dim] 处理后 每个z[a,b,:]向量都分别被变为长度为1\n",
    "    \n",
    "    z_ori = z[:, 0]  # n,z\n",
    "    z_trans = z[:, 1:]  # n,k-1, z\n",
    "    batch_size, num_trans, z_dim = z.shape  # 此处的num_trans实际上是num_trans+1\n",
    "\n",
    "    sim_matrix = torch.exp(torch.matmul(z, z.permute(0, 2, 1) / temp))  # n,k,k\n",
    "    \n",
    "    # 下面这一行 减号前的一项形状为[batch_size,num_trans+1,z_dim] 减号后的一项形状为[1,num_trans+1,z_dim]\n",
    "    # torch.eye()对角矩阵\n",
    "    mask = (torch.ones_like(sim_matrix).to(device) - torch.eye(num_trans).unsqueeze(0).to(device)).bool()\n",
    "    # 下面这一行 masked_select返回一维tensor\n",
    "    sim_matrix = sim_matrix.masked_select(mask).view(batch_size, num_trans, -1)\n",
    "    trans_matrix = sim_matrix[:, 1:].sum(-1)  # n,k-1\n",
    "\n",
    "    pos_sim = torch.exp(torch.sum(z_trans * z_ori.unsqueeze(1), -1) / temp) # n,k-1\n",
    "    K = num_trans - 1\n",
    "    scale = 1 / abs(K*float(torch.log(torch.tensor(1.0 / K))))\n",
    "\n",
    "    # loss_tensor = (torch.log(trans_matrix+1e-7) - torch.log(pos_sim+1e-7)) * scale\n",
    "    p_k=pos_sim/(trans_matrix+1e-7) # n,k-1\n",
    "    # y原本的形状是[n]\n",
    "    y=y.view(-1,1)\n",
    "    p_k_y=(1-y)*p_k+y*(1-p_k)\n",
    "    l_tensor =-torch.log(p_k_y+1e-7)\n",
    "    return l_tensor.sum(1)\n",
    "\n",
    "def e_loss(x,net,device):\n",
    "    # 测试时 仅使用损失函数的一项 需要传输全为0的y作为参数\n",
    "    y=torch.zeros(x.shape[0]).float()\n",
    "    y=y.to(device)\n",
    "    net = net.to(device)\n",
    "    x=x.to(device)\n",
    "    with torch.no_grad():\n",
    "        z=net(x)\n",
    "        l=NeuTraL_loss(z,device,y)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model(m1, m2):\n",
    "    # 更改m2的模型参数 赋值为模型m1的参数\n",
    "    d = OrderedDict()\n",
    "    for k in m1.state_dict().keys():\n",
    "        d[k] = m1.state_dict()[k].clone()\n",
    "    m2.load_state_dict(d)\n",
    "\n",
    "def FedAvg(model_list,data_num):\n",
    "    # 输入一个模型列表 返回聚合结果(字典)\n",
    "    total_num = sum(data_num)\n",
    "    d = OrderedDict()\n",
    "    for k in model_list[0].state_dict().keys():\n",
    "        d[k] = model_list[0].state_dict()[k].cpu().clone()*(data_num[0]/total_num)\n",
    "    for i in range(1, len(model_list)):\n",
    "        for k in model_list[0].state_dict().keys():\n",
    "            d[k] += model_list[i].state_dict()[k].cpu().clone()*(data_num[i]/total_num)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(m1,m2):\n",
    "    # 返回一个m2-m1的OrderedDict\n",
    "    d = OrderedDict()\n",
    "    for k in m1.state_dict().keys():\n",
    "        d[k] = m2.state_dict()[k].clone()-m1.state_dict()[k].clone()\n",
    "    return d\n",
    "\n",
    "def calculate_gradient_plus(m1,m2):\n",
    "    # 返回一个m1+m2的OrderedDict\n",
    "    d = OrderedDict()\n",
    "    for k in m1.state_dict().keys():\n",
    "        d[k] = m1.state_dict()[k].clone()+m2.state_dict()[k].clone()\n",
    "    return d\n",
    "\n",
    "def model_cosine_similarity(a,b):\n",
    "    global device\n",
    "    \n",
    "    ta=torch.tensor([]).to(device)\n",
    "    tb=torch.tensor([]).to(device)\n",
    "    for k in a.state_dict().keys():\n",
    "        \n",
    "        ta=torch.cat((ta,a.state_dict()[k].clone().view(-1)),dim=0)\n",
    "        tb=torch.cat((tb,b.state_dict()[k].clone().view(-1)),dim=0)\n",
    "    s=torch.cosine_similarity(ta,tb,dim=0)\n",
    "    return s.item()\n",
    "\n",
    "def model_distance_square(a,b):\n",
    "    global device\n",
    "    \n",
    "    t=torch.tensor([]).to(device)\n",
    "    for k in a.state_dict().keys():\n",
    "\n",
    "        dk=a.state_dict()[k].clone()-b.state_dict()[k].clone()\n",
    "        t=torch.cat((t, dk.view(-1)),dim=0)\n",
    "    square = t * t\n",
    "    return square.sum().item()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "def calculate_model_length_square(m):\n",
    "    global device\n",
    "    # 计算模型的长度的平方\n",
    "    t = torch.tensor([]).to(device)\n",
    "    for i in m.state_dict().keys():\n",
    "        t = torch.cat((t, m.state_dict()[i].view(-1)), 0)\n",
    "    square = t*t\n",
    "    return square.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2906d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9aa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02996764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义了3个集群的7个客户端数据配置，前6个是下层客户端，最后一个是上层客户端。\n",
    "# 每个客户端指定了:（污染数据集的正常样本数量，污染攻击样本类型及数量），（小型干净数据集的正常样本数量，攻击样本类型及数量）\n",
    "# ['DNS', 'LDAP', 'MSSQL', 'NetBIOS', 'NTP', 'Portmap', 'SNMP', 'SSDP', 'Syn', 'TFTP', 'UDPLag', 'UDP']\n",
    "dirty_num=0\n",
    "\n",
    "cluster1=[\n",
    "    ((3500-dirty_num,{'SNMP':0,'Portmap':0,'UDPLag': 0}),(0,{})),\n",
    "    ((3500-dirty_num,{'SNMP':0,'Portmap':0,'UDPLag': 0}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-dirty_num,{'SNMP':0,'Portmap':0,'UDPLag': 0}),\n",
    "     # (30,{'NetBIOS': 10,'Syn':10,'LDAP': 10})\n",
    "     (0,{'NetBIOS': 10,'Syn':10,'LDAP': 10})\n",
    "    ),\n",
    "]\n",
    "\n",
    "cluster2=[\n",
    "    ((3500-dirty_num,{'SNMP':0,'Portmap':0,'UDPLag': 0}),(0,{})),\n",
    "    ((3500-dirty_num,{'SNMP':0,'Portmap':0,'UDPLag': 0}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),\n",
    "     # (30,{'NetBIOS': 10,'Syn':10,'LDAP': 10})\n",
    "     (0,{'NetBIOS': 10,'Syn':10,'LDAP': 10})\n",
    "    ),\n",
    "]\n",
    "\n",
    "cluster3=[\n",
    "    ((3500-dirty_num,{'SNMP':0,'Portmap':0,'UDPLag': 0}),(0,{})),\n",
    "    ((3500-dirty_num,{'SNMP':0,'Portmap':0,'UDPLag': 0}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),(0,{})),\n",
    "    ((3500-0,{}),\n",
    "     # (30,{'NetBIOS': 10,'Syn':10,'LDAP': 10})\n",
    "     (0,{'NetBIOS': 10,'Syn':10,'LDAP': 10})\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 'SNMP':0,'Portmap':0,'UDPLag': 0\n",
    "# 'SNMP':24,'Portmap':23,'UDPLag': 23\n",
    "# 'SNMP':47,'Portmap':47,'UDPLag': 46\n",
    "# 'SNMP':70,'Portmap':70,'UDPLag': 70\n",
    "# 'SNMP':94,'Portmap':93,'UDPLag': 93\n",
    "# 'SNMP':117,'Portmap':117,'UDPLag': 116\n",
    "\n",
    "len(cluster1),len(cluster2),len(cluster3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_data=[\n",
    "    [load_client_data(cluster1[i][0],cluster1[i][1]) for i in range(len(cluster1))],\n",
    "    [load_client_data(cluster2[i][0],cluster2[i][1]) for i in range(len(cluster2))],\n",
    "    [load_client_data(cluster3[i][0],cluster3[i][1]) for i in range(len(cluster3))],\n",
    "]\n",
    "\n",
    "for p_cd in client_data:\n",
    "    print([(i[0][0].shape,i[0][1].shape,i[1][0].shape,i[1][1].shape) for i in p_cd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_num=4\n",
    "local_model_list=[\n",
    "    [NeuTraL(t_num) for i in range(len(cluster1))],\n",
    "    [NeuTraL(t_num) for i in range(len(cluster2))],\n",
    "    [NeuTraL(t_num) for i in range(len(cluster3))],\n",
    "]\n",
    "cluster_global_model_list=[NeuTraL(t_num) for i in range(len(client_data))]\n",
    "global_model=NeuTraL(t_num)\n",
    "\n",
    "lr=0.01\n",
    "weight_decay=0\n",
    "num_epochs=800\n",
    "batch_size=1024\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7f4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad168dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4ff20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single(model,X,y, lr,weight_decay, device):\n",
    "    model=model.to(device)\n",
    "    X=X.to(device)\n",
    "    y=y.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    z = model(X)\n",
    "    l_mean = NeuTraL_loss(z, device,y).mean()\n",
    "    optimizer.zero_grad()\n",
    "    l_mean.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def train_cluster(local_model_list,cluster_global_model,\n",
    "                  cluster_data, lr,weight_decay, device,batch_size):\n",
    "    # 训练某个cluster一个epoch\n",
    "    num_client=len(local_model_list)\n",
    "    cluster_global_model=cluster_global_model.to(device)\n",
    "    for i in range(num_client-1):\n",
    "        local_model_list[i]=local_model_list[i].to(device)\n",
    "        copy_model(cluster_global_model, local_model_list[i])\n",
    "        \n",
    "        # 训练普通client\n",
    "        s=random.sample(range(cluster_data[i][0][1].shape[0]), k=batch_size)\n",
    "        X=cluster_data[i][0][0][s]\n",
    "        y=torch.zeros(batch_size).float()\n",
    "        train_single(local_model_list[i],X,y,lr,weight_decay, device)\n",
    "\n",
    "    # 训练中间层client 即local_model_list的最后一个\n",
    "    local_model_list[-1]=local_model_list[-1].to(device)\n",
    "    copy_model(cluster_global_model, local_model_list[-1])\n",
    "    s=random.sample(range(cluster_data[-1][0][1].shape[0]), k=batch_size-cluster_data[-1][1][0].shape[0])\n",
    "    X=cluster_data[-1][0][0][s]\n",
    "    y=torch.zeros(batch_size-cluster_data[-1][1][0].shape[0]).float()\n",
    "    \n",
    "    X=torch.cat((X,cluster_data[-1][1][0]),dim=0)\n",
    "    y=torch.cat((y,torch.ones(cluster_data[-1][1][0].shape[0])),dim=0).float()\n",
    "    \n",
    "    train_single(local_model_list[-1],X,y,lr,weight_decay, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b48de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hierarchical(local_model_list,cluster_global_model_list,global_model,\n",
    "                       client_data,lr,weight_decay, device,batch_size,\n",
    "                       num_epochs,agg_f,print_e=5):\n",
    "    print(\"training on\", device)\n",
    "    start=time.time()\n",
    "    num_cluster=len(client_data)\n",
    "    \n",
    "    global_model=global_model.to(device)\n",
    "    for j in range(num_cluster):\n",
    "        cluster_global_model_list[j]=cluster_global_model_list[j].to(device)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        for j in range(num_cluster):\n",
    "            # 将global_model复制到当前cluster\n",
    "            copy_model(global_model, cluster_global_model_list[j])\n",
    "            # 逐一训练当前cluster的每个client\n",
    "            train_cluster(local_model_list[j],cluster_global_model_list[j],\n",
    "                          client_data[j],lr,weight_decay, device,batch_size)\n",
    "        for j in range(num_cluster):\n",
    "            # 当前cluster完成内部聚合 使用agg_f\n",
    "            c_g_d=agg_f(local_model_list[j],global_model,i,j)\n",
    "            cluster_global_model_list[j].load_state_dict(c_g_d)\n",
    "\n",
    "        # 完成cluster间聚合 使用FedAvg\n",
    "        global_d=FedAvg(cluster_global_model_list,[1]*num_cluster)\n",
    "        global_model.load_state_dict(global_d)\n",
    "\n",
    "        if (i+1)%print_e==0:\n",
    "            print(\"epoch\",i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c4bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1872e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8d451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trimmed_Mean_3(model_list,global_model,epoch,cluster_id):\n",
    "    c=3\n",
    "\n",
    "    num = len(model_list)\n",
    "    result_d = OrderedDict()\n",
    "    list_keys = list(model_list[0].state_dict().keys())\n",
    "    for i in list_keys:\n",
    "        shape_i = model_list[0].state_dict()[i].shape\n",
    "        tensor_i = [model_list[k].state_dict()[i].view(1, -1).cpu() for k in range(num)]\n",
    "        l1 = tensor_i[0].shape[1]\n",
    "        tensor_i_cat = torch.tensor([]).float().view(0, l1)\n",
    "        for t_i in tensor_i:\n",
    "            tensor_i_cat = torch.cat((tensor_i_cat, t_i), 0)\n",
    "        tensor_i_cat = torch.sort(tensor_i_cat, dim=0)\n",
    "        # 上一行返回的tensor_i_cat包含两项 第一项是排序结果 第二项是对应的下标(在这里用不到)\n",
    "        tensor_i_cat = tensor_i_cat[0]\n",
    "        result_i = torch.tensor([0] * l1).float()\n",
    "        for j in range(l1):\n",
    "            mean = tensor_i_cat[c:num - c, j].sum().item() / (num - 2 * c)\n",
    "            result_i[j] = mean\n",
    "        result_d[i] = result_i.view(shape_i)\n",
    "    return result_d\n",
    "\n",
    "def Krum_3(model_list,global_model,epoch,cluster_id):\n",
    "    c=3\n",
    "    \n",
    "    num = len(model_list)\n",
    "    distance = [[0] * num for i in range(num)]\n",
    "    for i in range(num - 1):\n",
    "        for j in range(i + 1, num):\n",
    "            t = model_distance_square(model_list[i], model_list[j])\n",
    "            distance[i][j] = t\n",
    "            distance[j][i] = t\n",
    "    min_score = float(\"inf\")\n",
    "    min_score_i = -1\n",
    "    for i in range(num):\n",
    "        distance[i].sort()\n",
    "        score_i = sum(distance[i][1:num - c - 1])\n",
    "        if score_i < min_score:\n",
    "            min_score = score_i\n",
    "            min_score_i = i\n",
    "    \n",
    "    print(min_score_i,end='\\t\\t')\n",
    "    result_d = OrderedDict()\n",
    "    for k in model_list[min_score_i].state_dict().keys():\n",
    "        result_d[k] = model_list[min_score_i].state_dict()[k].clone()\n",
    "    return result_d\n",
    "\n",
    "def FLTrust(model_list,global_model,epoch,cluster_id):\n",
    "    # 使用中间层参与方的小型干净数据集里的正常流量训练一个梯度作为基准 其不纳入聚合\n",
    "    global client_data\n",
    "    global dirty_num\n",
    "    t_m=NeuTraL(t_num).to(device)\n",
    "    copy_model(global_model, t_m)\n",
    "    if dirty_num==0:\n",
    "        # 训练集里的正常流量只有73500条 dirty_num=0即污染比例为0时只够正常数据集使用\n",
    "        # 故此时小型干净数据集里的数据使用30条正常数据集的代替 (小型干净数据集的正常流量数据只有FLTrust要用)\n",
    "        X=client_data[cluster_id][-1][0][0][-30:]\n",
    "        print('dirty_num=0!!!!!!!!!!!!!!!',X.shape)\n",
    "    else:\n",
    "        X=client_data[cluster_id][-1][1][1]\n",
    "    y=torch.zeros(X.shape[0]).float()\n",
    "    train_single(t_m,X,y,lr,weight_decay, device)\n",
    "    d=calculate_gradient(global_model,t_m)\n",
    "    t_m.load_state_dict(d)\n",
    "\n",
    "    num=len(model_list)\n",
    "    g_list=[NeuTraL(t_num) for i in range(num)]\n",
    "    for i in range(num):\n",
    "        g_list[i]=g_list[i].to(device)\n",
    "        d0=calculate_gradient(global_model,model_list[i])\n",
    "        g_list[i].load_state_dict(d0)\n",
    "    cs_l=[model_cosine_similarity(g_list[i],t_m) for i in range(num)]\n",
    "    TS=[max(0,cs_l[i]) for i in range(num)]\n",
    "    sum_TS=sum(TS)\n",
    "    g_length=[calculate_model_length_square(g_list[i])**0.5 for i in range(num)]\n",
    "    g0_l=calculate_model_length_square(t_m)**0.5\n",
    "    \n",
    "    d=OrderedDict()\n",
    "    for k in global_model.state_dict().keys():\n",
    "        t=g_list[0].state_dict()[k].clone()\n",
    "        d[k]=t*(g0_l*TS[0]/g_length[0])/sum_TS\n",
    "        for j in range(1,num):\n",
    "            t=g_list[j].state_dict()[k].clone()\n",
    "            d[k]+=t*(g0_l*TS[j]/g_length[j])/sum_TS\n",
    "\n",
    "        # 以上的d是一个模型梯度 需要变回模型\n",
    "        d[k]+=global_model.state_dict()[k].clone()\n",
    "\n",
    "    print(cs_l)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AVG(model_list,global_model,epoch,cluster_id):\n",
    "    # 不做任何防御措施\n",
    "    return FedAvg(model_list,data_num=[1]*len(model_list))\n",
    "\n",
    "global_c_s_info=[[2,-2],[2,-2],[2,-2],0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c38004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_s_based_type3(model_list,global_model,epoch,cluster_id):\n",
    "    global client_data\n",
    "    global global_c_s_info\n",
    "    global local_model_list\n",
    "    \n",
    "    def get_th_type3(n,a):\n",
    "        u,v=torch.std_mean(torch.tensor(a))\n",
    "        return float(v)+2*float(u)\n",
    "\n",
    "    num_cluster=len(client_data) #获取客户端数据的数量，即客户端的个数（通常是聚合的参与方）\n",
    "\n",
    "    if global_c_s_info[-1]<epoch:\n",
    "        # 模拟多个中间层client交换余弦相似度信息\n",
    "        for i in range(num_cluster):\n",
    "            t_base=NeuTraL(t_num).to(device)\n",
    "            copy_model(local_model_list[i][-1], t_base)\n",
    "            d=calculate_gradient(global_model,t_base)   #算该模型与全局模型的梯度\n",
    "            t_base.load_state_dict(d)\n",
    "\n",
    "            # 生成干净梯度\n",
    "            t_m=NeuTraL(t_num).to(device)\n",
    "            copy_model(global_model, t_m)\n",
    "            k1=1000-20\n",
    "            k2=20\n",
    "            s_n=random.choices(range(client_data[i][-1][0][0].shape[0]), k=k1)\n",
    "            s_a=random.choices(range(client_data[i][-1][1][0].shape[0]), k=k2)\n",
    "            X=torch.cat((client_data[i][-1][0][0][s_n],client_data[i][-1][1][0][s_a]),dim=0)\n",
    "            y1=torch.cat((torch.zeros(k1).float(),torch.ones(k2).float()),dim=0)\n",
    "            train_single(t_m,X,y1,lr,weight_decay, device)\n",
    "            d=calculate_gradient(global_model,t_m)\n",
    "            t_m.load_state_dict(d)\n",
    "            global_c_s_info[i][0]=model_cosine_similarity(t_base,t_m)\n",
    "            \n",
    "            # 生成污染梯度\n",
    "            y2=torch.zeros(k1+k2).float()\n",
    "            copy_model(global_model, t_m)\n",
    "            train_single(t_m,X,y2,lr,weight_decay, device)\n",
    "            d=calculate_gradient(global_model,t_m)\n",
    "            t_m.load_state_dict(d)\n",
    "            global_c_s_info[i][1]=model_cosine_similarity(t_base,t_m)\n",
    "\n",
    "        # 把global_c_s_info[-1]更新为当前epoch数\n",
    "        global_c_s_info[-1]=epoch\n",
    "        global_c_s_info[-2]=get_th_type3([global_c_s_info[i][0] for i in range(num_cluster)],  #所有cluster中所有客户端\n",
    "                                         [global_c_s_info[i][1] for i in range(num_cluster)])\n",
    "        print(global_c_s_info)\n",
    "\n",
    "    agg_list=[]\n",
    "    t_base=NeuTraL(t_num).to(device)\n",
    "    copy_model(model_list[-1], t_base)\n",
    "    d=calculate_gradient(global_model,t_base)\n",
    "    t_base.load_state_dict(d)\n",
    "    c_s_th=global_c_s_info[-2]\n",
    "    for i in range(len(model_list)-1):\n",
    "        t_m=NeuTraL(t_num).to(device)\n",
    "        copy_model(model_list[i], t_m)\n",
    "        d=calculate_gradient(global_model,t_m)\n",
    "        t_m.load_state_dict(d)\n",
    "        if model_cosine_similarity(t_base,t_m)>c_s_th:\n",
    "            agg_list.append(i)\n",
    "    agg_list.append(-1)\n",
    "    print(agg_list,c_s_th)\n",
    "    return FedAvg([model_list[i] for i in agg_list],data_num=[1]*len(agg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c180bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFA(model_list,global_model,epoch,cluster_id):\n",
    "    # RFA算法\n",
    "    \n",
    "    def geometric_median_objective(median, points, alphas):\n",
    "        \"\"\"Compute geometric median objective.\"\"\"\n",
    "        # median是一个点\n",
    "        # points是点列表\n",
    "        # alphas是权值列表\n",
    "        res=[]\n",
    "        for i in range(len(points)):\n",
    "            res.append(alphas[i]*(model_distance_square(median,points[i])**0.5))\n",
    "        return sum(res)\n",
    "    \n",
    "    num_m=len(model_list)\n",
    "    alphas=[1/num_m for i in range(num_m)]  # 各参与方的训练样本数相等\n",
    "\n",
    "    # 以fedavg的结果作为迭代初始值\n",
    "    median_d = FedAvg(model_list,data_num=alphas)\n",
    "    median=NeuTraL(t_num).to(device)\n",
    "    median.load_state_dict(median_d)\n",
    "\n",
    "    obj_val = geometric_median_objective(median, model_list, alphas)\n",
    "    \n",
    "    # 采用maxiter=4, eps=1e-5, ftol=1e-6\n",
    "    for i in range(4):\n",
    "        prev_median, prev_obj_val = median, obj_val\n",
    "        weights=[alphas[i] / max(1e-5, (model_distance_square(median,model_list[i])**0.5)) for i in range(num_m)]\n",
    "        \n",
    "        median_d = FedAvg(model_list,data_num=weights)\n",
    "        median.load_state_dict(median_d)\n",
    "        \n",
    "        obj_val = geometric_median_objective(median, model_list, alphas)\n",
    "        if abs(prev_obj_val - obj_val) < 1e-6 * obj_val:\n",
    "            break\n",
    "    \n",
    "    d = OrderedDict()\n",
    "    for k in median.state_dict().keys():\n",
    "        d[k] = median.state_dict()[k].clone()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dic={'Krum_3':Krum_3, 'Trimmed_Mean_3':Trimmed_Mean_3, 'AVG':AVG, \n",
    "         'FLTrust':FLTrust,'RFA':RFA,'c_s_based_type3':c_s_based_type3}\n",
    "\n",
    "for k in agg_dic.keys():\n",
    "    \n",
    "    if 'c_s_based' in k:\n",
    "        print(k,'change global_c_s_info')\n",
    "        global_c_s_info=[[2,-2],[2,-2],[2,-2],0,-1]\n",
    "    \n",
    "    local_model_list=[[NeuTraL(t_num) for i in range(len(cluster1))],\n",
    "                      [NeuTraL(t_num) for i in range(len(cluster2))],\n",
    "                      [NeuTraL(t_num) for i in range(len(cluster3))],\n",
    "                     ]\n",
    "    cluster_global_model_list=[NeuTraL(t_num) for i in range(len(client_data))]\n",
    "    global_model=NeuTraL(t_num)\n",
    "    \n",
    "    print(k,'------------------------------------------------------------------------')\n",
    "    \n",
    "    train_hierarchical(local_model_list,cluster_global_model_list,global_model,\n",
    "                       client_data,lr,weight_decay, device,batch_size,num_epochs,\n",
    "                       agg_f=agg_dic[k],\n",
    "                       print_e=50)\n",
    "    \n",
    "    \n",
    "    e_name=k+'-D'+str(dirty_num)\n",
    "    os.makedirs('./try1_result/'+e_name+'/')\n",
    "    save_path=\"./try1_result/\"+e_name+\"/model.pt\"\n",
    "    torch.save(global_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50081a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
